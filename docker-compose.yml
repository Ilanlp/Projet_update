# ===========================================
# JOBMARKET INFRASTRUCTURE
# ===========================================

x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.8.1}
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ""
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
    AIRFLOW__CORE__LOAD_EXAMPLES: "true"
    AIRFLOW__API__AUTH_BACKEND: "airflow.api.auth.backend.basic_auth"
    AIRFLOW__SMTP__SMTP_HOST: "smtp.gmail.com"
    AIRFLOW__SMTP__SMTP_PORT: 587
    AIRFLOW__SMTP__SMTP_USER: "de.airflow@gmail.com"
    AIRFLOW__SMTP__SMTP_PASSWORD: "cfsrvkongsobheta"
    AIRFLOW__SMTP__SMTP_MAIL_FROM: "de.airflow@gmail.com"

    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:- pymongo apache-airflow-providers-docker}
  volumes:
    - ./airflow/config:/opt/airflow/config
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./data:/opt/airflow/data
    - /var/run/docker.sock:/var/run/docker.sock
  user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}"
  depends_on:
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy
  profiles:
    - airflow
    - development
    - production

services:
  # ===========================================
  # ETL/ELT SERVICES
  # ===========================================

  jm-etl-normalizer:
    image: jm-etl-normalizer
    container_name: jm-etl-normalizer
    build:
      context: ./pipeline/src/
      dockerfile: Dockerfile.normalizer
      args:
        - USER_UID=${USER_UID:-1000}
        - USER_GID=${USER_GID:-1000}
    env_file:
      - ./pipeline/src/.env
    environment:
      - USER_UID=${USER_UID:-1000}
      - USER_GID=${USER_GID:-1000}
    volumes:
      - ./pipeline/src/data:/app/data:consistent
    profiles:
      - init-db

  jm-elt-snowflake:
    image: jm-elt-snowflake
    container_name: jm-elt-snowflake
    build:
      context: ./pipeline/src/
      dockerfile: Dockerfile.snowflake
      args:
        - USER_UID=${USER_UID:-1000}
        - USER_GID=${USER_GID:-1000}
    # networks:
    #   - jobmarket_network
    env_file:
      - ./pipeline/src/.env
    environment:
      - USER_UID=${USER_UID:-1000}
      - USER_GID=${USER_GID:-1000}
    volumes:
      - ./pipeline/src/snowflake:/usr/src/snowflake:consistent
      - ./pipeline/src/data:/usr/src/data:consistent
    profiles:
      - init-db

  jm-elt-dbt:
    image: jm-elt-dbt
    container_name: jm-elt-dbt
    build:
      context: ./snowflake/DBT/
      dockerfile: Dockerfile
      args:
        - USER_UID=${USER_UID:-1000}
        - USER_GID=${USER_GID:-1000}
    user: node
    # networks:
    #   - jobmarket_network
    env_file:
      - ./snowflake/DBT/.env
    environment:
      - USER_UID=${USER_UID:-1000}
      - USER_GID=${USER_GID:-1000}
    volumes:
      - ./snowflake/DBT:/usr/src/DBT:consistent
    depends_on:
      - jm-elt-snowflake
    profiles:
      - init-db

  # ===========================================
  # DASHBOARD
  # ===========================================

  frontend:
    image: jm-frontend
    container_name: jm-frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile.dash
      args:
        - USER_UID=${USER_UID:-1000}
        - USER_GID=${USER_GID:-1000}
    ports:
      - "8080:8080"
    volumes:
      - ./frontend:/app:consistent
    env_file:
      - ./frontend/.env
    environment:
      - USER_UID=${USER_UID:-1000}
      - USER_GID=${USER_GID:-1000}
    profiles:
      - frontend
      - development
      - production
    restart: unless-stopped

  # ===========================================
  # API
  # ===========================================

  backend:
    image: jm-backend
    container_name: jm-backend
    build:
      context: ./backend
      dockerfile: Dockerfile
      args:
        - USER_UID=${USER_UID:-1000}
        - USER_GID=${USER_GID:-1000}
    ports:
      - "8081:8081"
    env_file:
      - ./backend/.env
    volumes:
      - ./backend:/usr/src/backend:consistent
    environment:
      - USER_UID=${USER_UID:-1000}
      - USER_GID=${USER_GID:-1000}
    profiles:
      - backend
      - frontend
      - development
      - production
    restart: unless-stopped

  # ===========================================
  # MLFLOW SERVICES
  # ===========================================

  mlflow-tracking:
    container_name: mlflow_tracking
    image: jm-mlflow-tracking
    build:
      context: ./MLFlow
      dockerfile: Dockerfile.mlflow.tracking
      args:
        - USER_UID=${USER_UID:-1000}
        - USER_GID=${USER_GID:-1000}
    ports:
      - "${MLFLOW_EXTERNAL_PORT:-5010}:5000"
    volumes:
      - ./MLFlow/mlruns:/app/mlruns:consistent
      - ./MLFlow/mlflow.db:/app/mlflow.db:consistent
      - ./data:/app/data:consistent
    working_dir: /app
    environment:
      - USER_UID=${USER_UID:-1000}
      - USER_GID=${USER_GID:-1000}
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=file:///app/mlruns
    command: >
      bash -c " echo 'üéØ Starting MLflow Tracking Server...' && /opt/venv/bin/mlflow server  --backend-store-uri sqlite:///mlflow.db --default-artifact-root file:///app/mlruns --host 0.0.0.0  --port 5000 "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://mlflow-tracking:5000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    profiles:
      - init-ml
      - development
      - production
    restart: unless-stopped

  mlflow-training:
    container_name: mlflow_training
    image: jm-mlflow-training
    build:
      context: ./MLFlow
      dockerfile: Dockerfile.jobmarket_ml
      args:
        - PYTHON_VERSION=${PYTHON_VERSION:-3.11.12}
        - MLFLOW_VERSION=${MLFLOW_VERSION:-2.22.0}
        - INSTALL_GPU_SUPPORT=${INSTALL_GPU_SUPPORT:-false}
        - USER_UID=${USER_UID:-1000}
        - USER_GID=${USER_GID:-1000}
    volumes:
      # Lecture seule - Code et configuration
      - ./MLFlow/src:/app/src:consistent
      - ./MLFlow/scripts:/app/scripts:consistent
      # - ./MLFlow/config:/app/config:ro,cached

      # √âcriture fr√©quente - Logs et cache
      # - ./MLFlow/logs:/app/logs:delegated
      # - ./MLFlow/models:/app/models:delegated
      # - ./MLFlow/cache:/app/cache:delegated

      # Donn√©es critiques - Tracking et donn√©es
      - ./MLFlow/mlruns:/app/mlruns:consistent
      - ./MLFlow/data:/app/data:consistent
    environment:
      - USER_UID=${USER_UID:-1000}
      - USER_GID=${USER_GID:-1000}
      - MLFLOW_TRACKING_URI=http://mlflow-tracking:5000
      - MLFLOW_ARTIFACT_ROOT=/app/mlruns
      - MLFLOW_EXPERIMENT_NAME=${EXPERIMENT_NAME:-apple_demand_experiment}
      - MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING=true
      - MODEL_NAME=${MODEL_NAME:-jobmarket}
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - OMP_NUM_THREADS=4
      - OPENBLAS_NUM_THREADS=4
    working_dir: /app
    depends_on:
      mlflow-tracking:
        condition: service_healthy
    profiles:
      - init-ml
      - training
    # Ressources optimis√©es pour l'entra√Ænement
    deploy:
      resources:
        limits:
          cpus: "${TRAINING_CPU_LIMIT:-4.0}"
          memory: ${TRAINING_MEMORY_LIMIT:-8G}
        reservations:
          cpus: "2.0"
          memory: 4G
    restart: unless-stopped

  mlflow-model:
    container_name: mlflow_model
    image: jm-mlflow-model
    build:
      context: ./MLFlow
      dockerfile: Dockerfile.mlflow.model
      args:
        - MLFLOW_VERSION=${MLFLOW_VERSION:-2.22.0}
        - INSTALL_EXTRAS=true
        - BUILD_ENV=production
        - MODEL_PORT=${MODEL_PORT:-5001}
        - PYTHON_VERSION=${PYTHON_VERSION:-3.9}
        - USER_UID=${USER_UID:-1000}
        - USER_GID=${USER_GID:-1000}
    ports:
      - "${MODEL_EXTERNAL_PORT:-8000}:${MODEL_PORT:-5001}"
    volumes:
      - ./MLFlow/mlruns:/app/mlruns:consistent
      - ./MLFlow/models:/app/models:consistent
    environment:
      - USER_UID=${USER_UID:-1000}
      - USER_GID=${USER_GID:-1000}
      - TRACKING_URI=http://mlflow-tracking:5000
      - MODEL_URI=runs:/4aa5ee5ebfa945eaa75d103cca270238/apple_demand_model
      - MODEL_PORT=5001
      - DEBUG=true
      - LOG_LEVEL=DEBUG
      - PYTHONUNBUFFERED=1
      - MLFLOW_TRACKING_URI=http://mlflow-tracking:5000
      - MLFLOW_ARTIFACT_ROOT=/app/mlruns
      - MLFLOW_EXPERIMENT_ID=2
    working_dir: /app
    depends_on:
      mlflow-tracking:
        condition: service_healthy
    profiles:
      - development
      - production
      - init-ml
    # Ressources pour serving
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 1G
    restart: unless-stopped

  # ===========================================
  # MONITORING
  # ===========================================

  # # Service de monitoring (optionnel)
  # monitoring:
  #   image: prom/prometheus:latest
  #   container_name: prometheus_monitoring
  #   ports:
  #     - "9090:9090"
  #   volumes:
  #     - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
  #   profiles:
  #     - monitoring
  #   restart: unless-stopped

  # ===========================================
  # SERVICES UTILITAIRES
  # ===========================================

  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always
    profiles:
      - airflow
      - development
      - production

  redis:
    image: redis:latest
    ports:
      - 6379:6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always
    profiles:
      - airflow
      - development
      - production

  # ===========================================
  # AIRFLOW SERVICES
  # ===========================================
  # Licensed to the Apache Software Foundation (ASF) under one
  # or more contributor license agreements.  See the NOTICE file
  # distributed with this work for additional information
  # regarding copyright ownership.  The ASF licenses this file
  # to you under the Apache License, Version 2.0 (the
  # "License"); you may not use this file except in compliance
  # with the License.  You may obtain a copy of the License at
  #
  #   http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing,
  # software distributed under the License is distributed on an
  # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  # KIND, either express or implied.  See the License for the
  # specific language governing permissions and limitations
  # under the License.
  #

  # Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.
  #
  # WARNING: This configuration is for local development. Do not use it in a production deployment.
  #
  # This configuration supports basic configuration using environment variables or an .env file
  # The following variables are supported:
  #
  # AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.
  #                                Default: apache/airflow:master-python3.8
  # AIRFLOW_UID                  - User ID in Airflow containers
  #                                Default: 50000
  # AIRFLOW_GID                  - Group ID in Airflow containers
  #                                Default: 50000
  #
  # Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode
  #
  # _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).
  #                                Default: airflow
  # _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).
  #                                Default: airflow
  # _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.
  #                                Default: ''
  #
  # Feel free to modify this file to suit your needs.
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - 8080:8080
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"',
        ]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always

  airflow-init:
    <<: *airflow-common
    command: version
    environment:
      AIRFLOW_UID: 0
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis-airflow:6379/0
      AIRFLOW__CORE__FERNET_KEY: ""
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./airflow/dags:/opt/airflow/dags:consistent
      - ./pipeline/src/data:/app/data:consistent
      - ./airflow/logs:/opt/airflow/logs:consistent
      - ./airflow/plugins:/opt/airflow/plugins:consistent
      - ./airflow/config:/opt/airflow/config:consistent
      - /var/run/docker.sock:/var/run/docker.sock
    profiles:
      - airflow
    user: "0"
    restart: always

# ===========================================
  # MONITORING SERVICES
  # ===========================================
  # Service Prometheus - Collecte et stocke les m√©triques
  prometheus:
    image: prom/prometheus
    volumes:
      - "./monitoring/prometheus/:/etc/prometheus/" # Configuration et donn√©es de Prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml" # Fichier de configuration principal
      - "--storage.tsdb.path=/prometheus" # Emplacement de stockage des donn√©es
      - "--web.console.libraries=/usr/share/prometheus/console_libraries" # Biblioth√®ques pour la console web
      - "--web.console.templates=/usr/share/prometheus/consoles" # Templates pour la console web
    ports:
      - 9090:9090 # Interface web de Prometheus
    links:
      - cadvisor:cadvisor # Lien vers cAdvisor pour les m√©triques Docker
      - alertmanager:alertmanager # Lien vers AlertManager pour les alertes
    depends_on:
      - cadvisor # D√©pendance √† cAdvisor
    restart: always
    profiles:
      - monitoring

  # Service Node Exporter - Collecte les m√©triques syst√®me
  node-exporter:
    image: prom/node-exporter
    volumes:
      - /proc:/host/proc:ro # Acc√®s en lecture seule aux informations syst√®me
      - /sys:/host/sys:ro # Acc√®s en lecture seule aux informations syst√®me
      - /:/rootfs:ro # Acc√®s en lecture seule au syst√®me de fichiers
      - /:/host:ro # Acc√®s en lecture seule au syst√®me de fichiers
    command:
      - "--path.rootfs=/host" # Chemin racine du syst√®me
      - "--path.procfs=/host/proc" # Chemin vers les informations processus
      - "--path.sysfs=/host/sys" # Chemin vers les informations syst√®me
      - --collector.filesystem.ignored-mount-points # Points de montage ignor√©s
      - "^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)"
    ports:
      - 9100:9100 # Port pour les m√©triques Node Exporter
    restart: always
    profiles:
      - monitoring

  # Service AlertManager - Gestion des alertes
  alertmanager:
    image: prom/alertmanager
    volumes:
      - "./monitoring/alertmanager/:/etc/alertmanager/" # Configuration des alertes
    ports:
      - 9093:9093 # Interface web d'AlertManager
    command:
      - "--config.file=/etc/alertmanager/config.yml" # Fichier de configuration
      - "--storage.path=/alertmanager" # Stockage des alertes
    restart: always
    profiles:
      - monitoring

  # Service cAdvisor - Collecte les m√©triques des conteneurs Docker
  cadvisor:
    image: gcr.io/cadvisor/cadvisor
    volumes:
      - /:/rootfs:ro # Acc√®s en lecture seule au syst√®me de fichiers
      - /var/run:/var/run:rw # Acc√®s en lecture/√©criture aux informations Docker
      - /sys:/sys:ro # Acc√®s en lecture seule aux informations syst√®me
      - /var/lib/docker/:/var/lib/docker:ro # Acc√®s en lecture seule aux donn√©es Docker
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - "9091:8080" # Port par d√©faut de cAdvisor (8080 interne expos√© sur 9091)
    command:
      - "--store_container_labels=true"
    restart: always
    deploy:
      mode: global # D√©ploiement sur tous les n≈ìuds
    profiles:
      - monitoring

  # Service Grafana - Interface de visualisation des m√©triques
  grafana:
    image: grafana/grafana
    user: "472" # Utilisateur non-root pour la s√©curit√©
    depends_on:
      - prometheus # D√©pendance √† Prometheus
    ports:
      - 3000:3000 # Interface web de Grafana
    volumes:
      - grafana_data:/var/lib/grafana # Donn√©es persistantes de Grafana
      - ./monitoring/grafana/provisioning/:/etc/grafana/provisioning/ # Configuration automatique
    env_file:
      - ./monitoring/grafana/config.monitoring # Variables d'environnement
    restart: always
    profiles:
      - monitoring

# ===========================================
# R√âSEAUX
# ===========================================
networks:
  default:
    driver: bridge
    name: ${DOCKER_NETWORK_NAME:-jobmarket_network}

# ===========================================
# VOLUMES
# ===========================================
volumes:
  mlflow_data:
  postgres-db-volume:
# ===========================================
# PROFILES
# ===========================================
# Profils disponibles :
# - training     : Service d'entra√Ænement ML
# - development  : Training + Model
# - monitoring   : Prometheus pour monitoring
# - postgres     : Base PostgreSQL au lieu de SQLite
# - cache        : Redis pour cache
# - docs         : Documentation avec MkDocs
# - airflow      : Services Airflow (webserver, scheduler, worker, etc.)

# Exemples d'utilisation :
# docker-compose --profile development up                    # Training + Model
# docker-compose --profile training up                       # Entra√Ænement seulement
# docker-compose --profile training --profile monitoring up  # Training + Monitoring
# docker-compose --profile postgres up mlflow-tracking         # MLflow avec PostgreSQL
# docker-compose --profile airflow up                           # Services Airflow
