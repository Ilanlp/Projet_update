# ===========================================
# JOBMARKET INFRASTRUCTURE
# ===========================================

services:
  # ===========================================
  # ETL SERVICES
  # ===========================================
  jm-etl-normalizer:
    image: jm-etl-normalizer
    container_name: jm-etl-normalizer
    build:
      context: ./pipeline/src/
      dockerfile: Dockerfile.normalizer
    env_file:
      - ./pipeline/src/.env
    volumes:
      - ./pipeline/src/data:/app/data
    profiles:
      - init-db

  jm-elt-snowflake:
    image: jm-elt-snowflake
    container_name: jm-elt-snowflake
    build:
      context: ./pipeline/src/
      dockerfile: Dockerfile.snowflake
    # networks:
    #   - jobmarket_network
    env_file:
      - ./pipeline/src/.env
    volumes:
      - ./pipeline/src/snowflake:/usr/src/snowflake
      - ./pipeline/src/data:/usr/src/data
    profiles:
      - init-db

  jm-elt-dbt:
    image: jm-elt-dbt
    container_name: jm-elt-dbt
    build:
      context: ./snowflake/DBT/
      dockerfile: Dockerfile
    user: node
    # networks:
    #   - jobmarket_network
    env_file:
      - ./snowflake/DBT/.env
    volumes:
      - ./snowflake/DBT:/usr/src/DBT:cached
    depends_on:
      - jm-elt-snowflake
    profiles:
      - init-db

  # ===========================================
  # FRONTEND : DASHBOARD
  # ===========================================
  frontend:
    image: jm-frontend
    container_name: jm-frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile.dash
    ports:
      - "8080:8080"
    volumes:
      - ./frontend:/app
    env_file:
      - ./frontend/.env
    profiles:
      - frontend
      - development
      - production
    restart: unless-stopped
    # networks:
    #   - jobmarket_network

  # ===========================================
  # BACKEND : API (FASTAPI)
  # ===========================================
  backend:
    image: jm-backend
    container_name: jm-backend
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8081:8081"
    env_file:
      - ./backend/.env
    volumes:
      - ./backend:/usr/src/backend
    profiles:
      - backend
      - frontend
      - development
      - production
    restart: unless-stopped
    # networks:
    #   - jobmarket_network

  # ===========================================
  # MLFLOW SERVICES
  # ===========================================
  mlflow-tracking:
    container_name: mlflow_tracking
    image: jm-mlflow-tracking
    build:
      context: ./MLFlow
      dockerfile: Dockerfile.mlflow.tracking
    ports:
      - "${MLFLOW_EXTERNAL_PORT:-5010}:5000"
    volumes:
      - ./MLFlow/mlruns:/app/mlruns:rw
      - ./MLFlow/mlflow.db:/app/mlflow.db:rw
      - ./MLFlow/data:/app/data:rw
    working_dir: /app
    environment:
      - USER_ID=${USER_ID:-1000}
      - GROUP_ID=${GROUP_ID:-1000}
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=file:///app/mlruns
    command: >
      bash -c " echo 'üéØ Starting MLflow Tracking Server...' && /opt/venv/bin/mlflow server  --backend-store-uri sqlite:///mlflow.db --default-artifact-root file:///app/mlruns --host 0.0.0.0  --port 5000 "
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://mlflow-tracking:5000/health" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    profiles:
      - init-ml
      - development
      - production
    restart: unless-stopped
    # networks:
    #   - jobmarket_network

  mlflow-training:
    container_name: mlflow_training
    image: jm-mlflow-training
    build:
      context: ./MLFlow
      dockerfile: Dockerfile.jobmarket_ml
      args:
        - PYTHON_VERSION=${PYTHON_VERSION:-3.11.12}
        - MLFLOW_VERSION=${MLFLOW_VERSION:-2.22.0}
        - INSTALL_GPU_SUPPORT=${INSTALL_GPU_SUPPORT:-false}
        - USER_ID=${USER_ID:-1000}
        - GROUP_ID=${GROUP_ID:-1000}
    volumes:
        # Lecture seule - Code et configuration
        # - ./MLFlow/src:/app/src:ro,cached
        # - ./MLFlow/config:/app/config:ro,cached
        # - ./MLFlow/scripts:/app/scripts:ro,cached

        # √âcriture fr√©quente - Logs et cache
        # - ./MLFlow/logs:/app/logs:delegated
        # - ./MLFlow/models:/app/models:delegated
        # - ./MLFlow/cache:/app/cache:delegated

        # Donn√©es critiques - Tracking et donn√©es
        - ./MLFlow/mlruns:/app/mlruns:consistent
        - ./MLFlow/data:/app/data:consistent
    environment:
      - USER_ID=${USER_ID:-1000}
      - GROUP_ID=${GROUP_ID:-1000}
      - MLFLOW_TRACKING_URI=http://mlflow-tracking:5000
      - MLFLOW_ARTIFACT_ROOT=/app/mlruns
      - MLFLOW_EXPERIMENT_NAME=${EXPERIMENT_NAME:-apple_demand_experiment}
      - MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING=true
      - MODEL_NAME=${MODEL_NAME:-jobmarket}
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - OMP_NUM_THREADS=4
      - OPENBLAS_NUM_THREADS=4
    working_dir: /app
    depends_on:
      mlflow-tracking:
        condition: service_healthy
    profiles:
      - init-ml
      - training
    # Ressources optimis√©es pour l'entra√Ænement
    deploy:
      resources:
        limits:
          cpus: "${TRAINING_CPU_LIMIT:-4.0}"
          memory: ${TRAINING_MEMORY_LIMIT:-8G}
        reservations:
          cpus: "2.0"
          memory: 4G
    restart: unless-stopped
    # networks:
    #   - jobmarket_network
    # ===========================================
    # MLFLOW MODEL
    # ===========================================
  mlflow-model:
    container_name: mlflow_model
    image: jm-mlflow-model
    build:
      context: ./MLFlow
      dockerfile: Dockerfile.mlflow.model
      args:
        - MLFLOW_VERSION=${MLFLOW_VERSION:-2.22.0}
        - INSTALL_EXTRAS=true
        - BUILD_ENV=production
        - MODEL_PORT=${MODEL_PORT:-5001}
        - PYTHON_VERSION=${PYTHON_VERSION:-3.9}
        - USER_ID=${USER_ID:-1000}
        - GROUP_ID=${GROUP_ID:-1000}
    ports:
      - "${MODEL_EXTERNAL_PORT:-8000}:${MODEL_PORT:-5001}"
    volumes:
      - ./MLFlow/mlruns:/app/mlruns:rw
      - ./MLFlow/models:/app/models:rw
    environment:
      - USER_ID=${USER_ID:-1000}
      - GROUP_ID=${GROUP_ID:-1000}
      - TRACKING_URI=http://mlflow-tracking:5000
      - MODEL_URI=runs:/4aa5ee5ebfa945eaa75d103cca270238/apple_demand_model
      - MODEL_PORT=5001
      - DEBUG=true
      - LOG_LEVEL=DEBUG
      - PYTHONUNBUFFERED=1
      - MLFLOW_TRACKING_URI=http://mlflow-tracking:5000
      - MLFLOW_ARTIFACT_ROOT=/app/mlruns
      - MLFLOW_EXPERIMENT_ID=2
    working_dir: /app
    depends_on:
      mlflow-tracking:
        condition: service_healthy
    profiles:
      - development
      - production
      - init-ml
    # Ressources pour serving
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 1G
    restart: unless-stopped
    # networks:
    #   - jobmarket_network
    # ===========================================
    # SERVICES UTILITAIRES
    # ===========================================

    # # Service de monitoring (optionnel)
    # monitoring:
    #   image: prom/prometheus:latest
    #   container_name: prometheus_monitoring
    #   ports:
    #     - "9090:9090"
    #   volumes:
    #     - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    #   profiles:
    #     - monitoring
    #   restart: unless-stopped

    # # Service de base de donn√©es avanc√©e (optionnel)
    # postgres:
    #   image: postgres:15-alpine
    #   container_name: postgres_mlflow
    #   environment:
    #     - POSTGRES_DB=${POSTGRES_DB:-mlflow}
    #     - POSTGRES_USER=${POSTGRES_USER:-mlflow}
    #     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-mlflow}
    #   volumes:
    #     - postgres_data:/var/lib/postgresql/data
    #   ports:
    #     - "5432:5432"
    #   profiles:
    #     - postgres
    #   restart: unless-stopped

    # # Service Redis pour cache (optionnel)
    # redis:
    #   image: redis:7-alpine
    #   container_name: redis_cache
    #   ports:
    #     - "6379:6379"
    #   volumes:
    #     - redis_data:/data
    #   profiles:
    #     - cache
    #   restart: unless-stopped

  # ===========================================
  # AIRFLOW SERVICES
  # ===========================================
  postgres-airflow:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-airflow-db-volume:/var/lib/postgresql/data
    profiles:
      - airflow
    restart: always

  redis-airflow:
    image: redis:7.2-bookworm
    expose:
      - 6379
    profiles:
      - airflow
    restart: always

  airflow-webserver:
    image: apache/airflow:2.7.3
    command: webserver
    depends_on:
      - postgres-airflow
      - redis-airflow
    environment:
      AIRFLOW_UID: 0
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis-airflow:6379/0
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    profiles:
      - airflow
    restart: always

  airflow-scheduler:
    image: apache/airflow:2.7.3
    command: scheduler
    depends_on:
      - postgres-airflow
      - redis-airflow
    environment:
      AIRFLOW_UID: 0
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis-airflow:6379/0
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - /var/run/docker.sock:/var/run/docker.sock
    profiles:
      - airflow
    restart: always

  airflow-worker:
    image: apache/airflow:2.7.3
    command: celery worker
    depends_on:
      - postgres-airflow
      - redis-airflow
    env_file:
      - ./pipeline/src/.env
    environment:
      AIRFLOW_UID: 0
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis-airflow:6379/0
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./pipeline/src/data:/app/data
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - /var/run/docker.sock:/var/run/docker.sock
    profiles:
      - airflow
    user: "0"
    restart: always

    # ===========================================
    # R√âSEAUX
    # ===========================================
networks:
  default:
    driver: bridge
    name: ${DOCKER_NETWORK_NAME:-jobmarket_network}

# ===========================================
# VOLUMES
# ===========================================
volumes:
  mlflow_data:
    driver: local
  postgres-airflow-db-volume:
    driver: local

# ===========================================
# PROFILES
# ===========================================
# Profils disponibles :
# - training     : Service d'entra√Ænement ML
# - development  : Training + Model
# - monitoring   : Prometheus pour monitoring
# - postgres     : Base PostgreSQL au lieu de SQLite
# - cache        : Redis pour cache
# - docs         : Documentation avec MkDocs
# - airflow      : Services Airflow (webserver, scheduler, worker, etc.)

# Exemples d'utilisation :
# docker-compose --profile development up                    # Training + Model
# docker-compose --profile training up                       # Entra√Ænement seulement
# docker-compose --profile training --profile monitoring up  # Training + Monitoring
# docker-compose --profile postgres up mlflow-tracking         # MLflow avec PostgreSQL
# docker-compose --profile airflow up                           # Services Airflow
